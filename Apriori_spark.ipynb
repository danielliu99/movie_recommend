{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import intersection\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/ext3/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/05/07 11:05:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# sc = SparkContext(appName=\"Spark Apriori\")\n",
    "conf = SparkConf().setAppName(\"Spark Apriori\").setMaster(os.getenv('SPARK_URL'))\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cm013.hpc.nyu.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://cm013:23708</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Apriori</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://cm013:23708 appName=Spark Apriori>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent K-item Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_c(f_k, k):\n",
    "    next_c = [var1 | var2 for index, var1 in enumerate(f_k) for var2 in f_k[index + 1:] if\n",
    "              list(var1)[:k - 2] == list(var2)[:k - 2]]\n",
    "    return next_c\n",
    "\n",
    "def generate_f_k(sc, c_k, shared_itemset, sup):\n",
    "    def get_sup(x):\n",
    "        x_sup = len([1 for t in shared_itemset.value if x.issubset(t)])\n",
    "        if x_sup >= sup:\n",
    "            return x, x_sup\n",
    "        else:\n",
    "            return ()\n",
    "\n",
    "    f_k = sc.parallelize(c_k).map(get_sup).filter(lambda x: x).collect()\n",
    "    return f_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(sc, f_input, f_output, min_support):\n",
    "    # read the raw data\n",
    "    data = sc.textFile(f_input)\n",
    "    # count the total number of samples\n",
    "    n_samples = data.count()\n",
    "    # min_support to frequency\n",
    "    min_support_count = n_samples * min_support\n",
    "    # split sort\n",
    "    itemset = data.map(lambda line: sorted([int(item) for item in line.strip().split(' ')]))\n",
    "    # share the whole itemset with all workers\n",
    "    shared_itemset = sc.broadcast(itemset.map(lambda x: set(x)).collect())\n",
    "    # store for all freq_k\n",
    "    frequent_itemset = []\n",
    "\n",
    "    # prepare candidate_1\n",
    "    k = 1\n",
    "    c_k = itemset.flatMap(lambda x: set(x)).distinct().collect()\n",
    "    c_k = [{x} for x in c_k]\n",
    "\n",
    "    # when candidate_k is not empty\n",
    "    while len(c_k) > 0:\n",
    "        # generate freq_k\n",
    "#         print(\"C{}: {}\".format(k, c_k))\n",
    "        f_k = generate_f_k(sc, c_k, shared_itemset, min_support_count)\n",
    "#         print(\"F{}: {}\".format(k, f_k))\n",
    "\n",
    "        frequent_itemset.append(f_k)\n",
    "        # generate candidate_k+1\n",
    "        k += 1\n",
    "        c_k = generate_next_c([set(item) for item in map(lambda x: x[0], f_k)], k)\n",
    "\n",
    "    if len(frequent_itemset[-1]) == 0: \n",
    "        frequent_itemset = frequent_itemset[:-1]\n",
    "    # output the result to file system\n",
    "    sc.parallelize(frequent_itemset, numSlices=1).saveAsTextFile(f_output)\n",
    "#     return sc.parallelize(frequent_itemset).map(lambda x: list(x[0])).collect()\n",
    "    return frequent_itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ./output/frequent_set/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_path = './dataset/apriori_ratings.txt'\n",
    "output_path = './output/frequent_set'\n",
    "min_support = 0.02\n",
    "freqset = apriori(sc, input_path, output_path, min_support)\n",
    "# apriori(sc, input_path, output_path, min_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 127\n",
      "2 467\n",
      "3 477\n",
      "4 176\n",
      "5 30\n",
      "6 1\n"
     ]
    }
   ],
   "source": [
    "for k, k_set in enumerate(freqset): \n",
    "    print(k+1, len(k_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence(sc, freq_parent, freq_child, conf): \n",
    "    def get_confidence(x): \n",
    "        result = [(x[0], child[0], round(child[1]/x[1], 2)) for child in freq_child.value if x[0].issubset(child[0]) and child[1]/x[1]>conf]\n",
    "        return result\n",
    "        \n",
    "    return sc.parallelize(freq_parent).flatMap(get_confidence).filter(lambda x:x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({41566}, 4604)\n",
      "({858, 1262}, 5903)\n",
      "({318, 858, 1262}, 4274)\n",
      "({4226, 2324, 858, 318}, 3763)\n",
      "({4226, 293, 858, 1213, 318}, 3639)\n"
     ]
    }
   ],
   "source": [
    "len(freqset)\n",
    "association_rules = []\n",
    "for k in range(len(freqset) - 1): \n",
    "    print(freqset[k][0])\n",
    "    freq_parent = freqset[k]\n",
    "    freq_child = sc.broadcast(freqset[k+1])\n",
    "    association_rules.append(confidence(sc, freq_parent, freq_child, conf = 0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize (store) the frequent set result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/association_rules', 'wb') as f:\n",
    "    pickle.dump(association_rules, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./output/association_rules', 'rb') as f:\n",
    "    rules = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
