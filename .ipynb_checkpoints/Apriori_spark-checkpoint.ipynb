{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9644a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9556c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/ext3/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/04/27 13:18:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Spark Context \n",
    "# Also extra coniguration can be added \n",
    "sc = SparkContext(\"local\" , \"Apriori\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fd4253c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cs471.hpc.nyu.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Apriori</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Apriori>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a675383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['858,1246,1968,4226', '605,628', '1645,1909,1911,2023,2355', '858,1945', '293,318', '318,1266,2019', '1246,4226,2355,497,1262,2324', '500,318,1073,1923,4995,51540', '858,1246,2355,293,318,497,590,1089,176,322,441,492,501,549,608,926,953,1213,1273,1537,1635,1639,1683,1883,1957,2020,2064,2095,2348,2657', '293,8961,1265,1262,608,745,4327,6947,7445,8874']\n"
     ]
    }
   ],
   "source": [
    "# Read file\n",
    "file =  sc.textFile(\"./dataset/apriori_ratings.txt\")\n",
    "file = file.map(lambda line: line.strip('\"'))\n",
    "print(file.collect()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a5962b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163556"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "595f5846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['858', '1246', '1968', '4226'], ['605', '628'], ['1645', '1909', '1911', '2023', '2355'], ['858', '1945'], ['293', '318'], ['318', '1266', '2019'], ['1246', '4226', '2355', '497', '1262', '2324'], ['500', '318', '1073', '1923', '4995', '51540'], ['858', '1246', '2355', '293', '318', '497', '590', '1089', '176', '322', '441', '492', '501', '549', '608', '926', '953', '1213', '1273', '1537', '1635', '1639', '1683', '1883', '1957', '2020', '2064', '2095', '2348', '2657'], ['293', '8961', '1265', '1262', '608', '745', '4327', '6947', '7445', '8874']]\n"
     ]
    }
   ],
   "source": [
    "## Each transaction to be a list: [a, b, c] \n",
    "transactions = file.map(lambda line: line.split(','))\n",
    "\n",
    "print(transactions.collect()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a630ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flat all transactions into one list\n",
    "flatTransactions = transactions.flatMap(lambda line: line)\n",
    "\n",
    "# flatTransactions.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "868b1e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get a list of all unique items \n",
    "uniqueItems = file.flatMap(lambda line:line.split(',')).distinct()\n",
    "\n",
    "# print(uniqueItems.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "725c1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Construct unique items as tuples -> (item, count)\n",
    "supportRdd = flatTransactions.map(lambda item: (item , 1))\n",
    "supportRdd = supportRdd.reduceByKey(add)\n",
    "# supportRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ece33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1-item set support values\n",
    "supports = supportRdd.map(lambda item: item[1])\n",
    "# supports.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7a8b84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Unique frequent items in dataset\n",
    "# uniqueItems = wlitems.distinct()\n",
    "\n",
    "# # Add 1 as Tuple\n",
    "# supportRdd = wlitems.map(lambda item: (item , 1))\n",
    "\n",
    "# # Method for sum in reduceByKey method\n",
    "# def sumOparator(x,y):\n",
    "#     return x+y\n",
    "\n",
    "# # Sum of values by key\n",
    "# supportRdd = supportRdd.reduceByKey(sumOparator)\n",
    "\n",
    "# print(supportRdd.collect()) # Retruns following array \n",
    "# [('Apple', 12), ('Mango', 10), ('Banana', 9), ('Coconut', 3), \n",
    "#  ('Strawberry', 4), ('Grapes', 2), ('Lemon', 1), ('Raspberry', 9), ('Rassberry', 1)]\n",
    "\n",
    "\n",
    "# # First support values\n",
    "# supports = supportRdd.map(lambda item: item[1]) # Return only support values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "691ebcf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Minimum support value\n",
    "minSupport = max(2, supports.min())\n",
    "minSupport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c45419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter first supportRdd with minimum support: (item, support)\n",
    "supportRdd = supportRdd.filter(lambda item: item[1] >= minSupport )\n",
    "# supportRdd.collect()\n",
    "\n",
    "## Create base RDD which will be updated every iteration\n",
    "## put item[0] in a list: ([a, b, c], support)\n",
    "baseRdd = supportRdd.map(lambda item: ([item[0]] , item[1])) \n",
    "# baseRdd.collect()\n",
    "\n",
    "## supportRdd: frequent k-item set\n",
    "supportRdd = supportRdd.map(lambda item: item[0]) # only store item set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2038bc3b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# # Define minimum support value \n",
    "# minSupport = supports.min()\n",
    "\n",
    "# # If mininmum support is 1 then replace it with 2 \n",
    "# minSupport = 2 if minSupport == 1 else minSupport\n",
    "\n",
    "# ## Filter first supportRdd with minimum support \n",
    "# supportRdd = supportRdd.filter(lambda item: item[1] >= minSupport )\n",
    "\n",
    "# ## Create base RDD which will be updated every iteration\n",
    "# baseRdd = supportRdd.map(lambda item: ([item[0]] , item[1])) # put item[0] in a list\n",
    "# print('1 . Table has crated...') \n",
    "\n",
    "# supportRdd = supportRdd.map(lambda item: item[0]) # only store itemset\n",
    "# supportRddCart = supportRdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ee340f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeReplica(record):\n",
    "\n",
    "    if(isinstance(record[0], tuple)):\n",
    "        x1 = record[0]\n",
    "        x2 = record[1]\n",
    "    else:\n",
    "        x1 = [record[0]]\n",
    "        x2 = record[1]\n",
    "\n",
    "    if(any(x == x2 for x in x1) == False):\n",
    "        a = list(x1)\n",
    "        a.append(x2)\n",
    "        a.sort()\n",
    "        result = tuple(a)\n",
    "        return result \n",
    "    else:\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6728fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 . Table has crated... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/state/partition1/job-32826626/ipykernel_3161610/955616949.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;31m# Combination length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupportRdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misEmpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m## cartesian product of frequent k-item set and unique items -> candidate (k+1)-item set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/pyspark/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36misEmpty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1604\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m         \"\"\"\n\u001b[0;32m-> 1606\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msaveAsNewAPIHadoopDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalueConverter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/pyspark/lib/python3.8/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1566\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/pyspark/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/ext3/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/pyspark/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ext3/pyspark/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c = 2 # Combination length \n",
    "\n",
    "while(supportRdd.isEmpty() == False):\n",
    "    \n",
    "    ## cartesian product of frequent k-item set and unique items -> candidate (k+1)-item set \n",
    "    combined = supportRdd.cartesian(uniqueItems) \n",
    "    combined = combined.map(lambda item: removeReplica(item))\n",
    "  \n",
    "    combined = combined.filter(lambda item: len(item) == c)\n",
    "    combined = combined.distinct()\n",
    "\n",
    "    \n",
    "    combined_2 = combined.cartesian(transactions)\n",
    "    combined_2 = combined_2.filter(lambda item: all(x in item[1] for x in item[0]))\n",
    "    \n",
    "#     combined_2 = combined_2.map(lambda item: item[0])\n",
    "    combined_2 = combined_2.map(lambda item: (item[0], 1))\n",
    "    combined_2 = combined_2.reduceByKey(add)\n",
    "    combined_2 = combined_2.filter(lambda item: item[1] >= minSupport)\n",
    "\n",
    "    baseRdd = baseRdd.union(combined_2)\n",
    "    \n",
    "    combined_2 = combined_2.map(lambda item: item[0])\n",
    "    supportRdd = combined_2\n",
    "    print(c ,'. Table has crated... ')\n",
    "#     print(combined_2.collect())\n",
    "    c = c+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca286f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>   (0 + 0) / 1][Stage 24:>   (0 + 0) / 1][Stage 25:>   (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "baseRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c488d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# : Aggregated support values preparing for the confidence calculatations\n",
      "# : Aggregated support values are ready !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 147:=========================================>             (12 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Before                After  Confidence\n",
      "0               [Apple]              [Mango]   58.333333\n",
      "1               [Mango]              [Apple]   70.000000\n",
      "2               [Apple]             [Banana]   41.666667\n",
      "3               [Apple]            [Coconut]   16.666667\n",
      "4              [Banana]              [Apple]   55.555556\n",
      "5             [Coconut]              [Apple]   66.666667\n",
      "6               [Apple]         [Strawberry]   25.000000\n",
      "7               [Apple]          [Raspberry]   33.333333\n",
      "8               [Mango]             [Banana]   60.000000\n",
      "9              [Banana]              [Mango]   66.666667\n",
      "10         [Strawberry]              [Apple]   75.000000\n",
      "11          [Raspberry]              [Apple]   40.000000\n",
      "12             [Grapes]          [Raspberry]  100.000000\n",
      "13          [Raspberry]             [Grapes]   20.000000\n",
      "14              [Mango]          [Raspberry]   40.000000\n",
      "15             [Banana]          [Raspberry]   44.444444\n",
      "16          [Raspberry]              [Mango]   40.000000\n",
      "17          [Raspberry]             [Banana]   40.000000\n",
      "18            [Coconut]          [Raspberry]   66.666667\n",
      "19          [Raspberry]            [Coconut]   20.000000\n",
      "20              [Apple]      [Mango, Banana]   33.333333\n",
      "21              [Mango]      [Apple, Banana]   40.000000\n",
      "22             [Banana]       [Mango, Apple]   44.444444\n",
      "23              [Apple]   [Mango, Raspberry]   16.666667\n",
      "24              [Mango]   [Raspberry, Apple]   20.000000\n",
      "25              [Mango]  [Raspberry, Banana]   20.000000\n",
      "26             [Banana]   [Mango, Raspberry]   22.222222\n",
      "27          [Raspberry]       [Mango, Apple]   20.000000\n",
      "28          [Raspberry]      [Mango, Banana]   20.000000\n",
      "29       [Mango, Apple]             [Banana]   57.142857\n",
      "30      [Apple, Banana]              [Mango]   80.000000\n",
      "31      [Mango, Banana]              [Apple]   66.666667\n",
      "32       [Mango, Apple]          [Raspberry]   28.571429\n",
      "33   [Raspberry, Apple]              [Mango]   50.000000\n",
      "34      [Mango, Banana]          [Raspberry]   33.333333\n",
      "35   [Mango, Raspberry]              [Apple]   50.000000\n",
      "36   [Mango, Raspberry]             [Banana]   50.000000\n",
      "37  [Raspberry, Banana]              [Mango]   50.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def filterForConf(item , total):\n",
    "    '''\n",
    "    item: [(((a,b,c), 3), ((a,c), 2)), ...]\n",
    "    this function filter `item` based on two conditions:\n",
    "    1. the left has more items than the right\n",
    "    2. any item in the right has a match in the left\n",
    "    '''\n",
    "    if(len(item[0][0]) > len(item[1][0])  ):\n",
    "        if(checkItemSets(item[0][0] , item[1][0]) == False):\n",
    "            pass\n",
    "        else:\n",
    "            return (item)       \n",
    "    else:\n",
    "        pass  \n",
    "\n",
    "    \n",
    "def checkItemSets(item_1 , item_2):\n",
    "\n",
    "    if(len(item_1) > len(item_2)):\n",
    "#             return all(right in item_1 for right in item_2)\n",
    "        return all(any(k == l for k in item_1 ) for l in item_2)\n",
    "    else:\n",
    "        return all(left in item_1 for left in item_1)\n",
    "#             return all(any(k == l for k in item_2 ) for l in item_1)\n",
    "\n",
    "\n",
    "def calculateConfidence(item):\n",
    "\n",
    "    # Parent item list\n",
    "    parent = set(item[0][0])\n",
    "\n",
    "    # Child item list\n",
    "    if(isinstance(item[1][0] , str)):\n",
    "        child  = set([item[1][0]])\n",
    "    else:\n",
    "        child  = set(item[1][0])\n",
    "    # Parent and Child support values\n",
    "    parentSupport = item[0][1]\n",
    "    childSupport = item[1][1]\n",
    "    # Finds the item set confidence is going to be found\n",
    "\n",
    "    support = (parentSupport / childSupport)*100\n",
    "\n",
    "    return list([ list(child) ,  list(parent.difference(child)) , support ])\n",
    "\n",
    "\n",
    "# Example ((('x10', 'x3', 'x6', 'x7', 'x9'), 1), (('x10', 'x3', 'x7'), 1))\n",
    "calcuItems = baseRdd.cartesian(baseRdd)\n",
    "\n",
    "#deneme = calcuItems.map(lambda item: lens(item)) \n",
    "total = calcuItems.count()\n",
    "\n",
    "print('# : Aggregated support values preparing for the confidence calculatations')\n",
    "baseRddConfidence = calcuItems.filter(lambda item: filterForConf(item , total))\n",
    "print('# : Aggregated support values are ready !')\n",
    "baseRddConfidence = baseRddConfidence.map(lambda item: calculateConfidence(item))\n",
    "\n",
    "  \n",
    "# print(baseRddConfidence.collect())\n",
    "\n",
    "## Create an array with collected baseRddConfidence results\n",
    "result = baseRddConfidence.collect()\n",
    "\n",
    "## Create Data Frame\n",
    "confidenceTable = pd.DataFrame(data = result , columns=[\"Before\", \"After\" , \"Confidence\"])\n",
    "\n",
    "## Show data frame\n",
    "print(confidenceTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e684343",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# : Aggregated support values preparing for the confidence calculatations\n",
      "# : Aggregated support values are ready !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:==========================================>             (12 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Before                After  Confidence\n",
      "0               [Apple]              [Mango]   58.333333\n",
      "1               [Mango]              [Apple]   70.000000\n",
      "2               [Apple]             [Banana]   41.666667\n",
      "3               [Apple]            [Coconut]   16.666667\n",
      "4              [Banana]              [Apple]   55.555556\n",
      "5             [Coconut]              [Apple]   66.666667\n",
      "6               [Apple]         [Strawberry]   25.000000\n",
      "7               [Apple]          [Raspberry]   33.333333\n",
      "8               [Mango]             [Banana]   60.000000\n",
      "9              [Banana]              [Mango]   66.666667\n",
      "10         [Strawberry]              [Apple]   75.000000\n",
      "11          [Raspberry]              [Apple]   40.000000\n",
      "12             [Grapes]          [Raspberry]  100.000000\n",
      "13          [Raspberry]             [Grapes]   20.000000\n",
      "14              [Mango]          [Raspberry]   40.000000\n",
      "15             [Banana]          [Raspberry]   44.444444\n",
      "16          [Raspberry]              [Mango]   40.000000\n",
      "17          [Raspberry]             [Banana]   40.000000\n",
      "18            [Coconut]          [Raspberry]   66.666667\n",
      "19          [Raspberry]            [Coconut]   20.000000\n",
      "20              [Apple]      [Mango, Banana]   33.333333\n",
      "21              [Mango]      [Apple, Banana]   40.000000\n",
      "22             [Banana]       [Mango, Apple]   44.444444\n",
      "23              [Apple]   [Mango, Raspberry]   16.666667\n",
      "24              [Mango]   [Raspberry, Apple]   20.000000\n",
      "25              [Mango]  [Raspberry, Banana]   20.000000\n",
      "26             [Banana]   [Mango, Raspberry]   22.222222\n",
      "27          [Raspberry]       [Mango, Apple]   20.000000\n",
      "28          [Raspberry]      [Mango, Banana]   20.000000\n",
      "29       [Mango, Apple]             [Banana]   57.142857\n",
      "30      [Apple, Banana]              [Mango]   80.000000\n",
      "31      [Mango, Banana]              [Apple]   66.666667\n",
      "32       [Mango, Apple]          [Raspberry]   28.571429\n",
      "33   [Raspberry, Apple]              [Mango]   50.000000\n",
      "34      [Mango, Banana]          [Raspberry]   33.333333\n",
      "35   [Mango, Raspberry]              [Apple]   50.000000\n",
      "36   [Mango, Raspberry]             [Banana]   50.000000\n",
      "37  [Raspberry, Banana]              [Mango]   50.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "class Filter():\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.stages = 1\n",
    "\n",
    "\n",
    "    def filterForConf(self, item , total):\n",
    "        \n",
    "        if(len(item[0][0]) > len(item[1][0])  ):\n",
    "            if(self.checkItemSets(item[0][0] , item[1][0]) == False):\n",
    "                pass\n",
    "            else:\n",
    "                return (item)       \n",
    "        else:\n",
    "            pass  \n",
    "        self.stages = self.stages + 1\n",
    "\n",
    "    # Check Items sets includes at least one comman item // Example command: # any(l == k for k in z for l in x )\n",
    "    def checkItemSets(self, item_1 , item_2):\n",
    "\n",
    "        if(len(item_1) > len(item_2)):\n",
    "            return all(any(k == l for k in item_1 ) for l in item_2)\n",
    "        else:\n",
    "            return all(any(k == l for k in item_2 ) for l in item_1)\n",
    "\n",
    "\n",
    "    def calculateConfidence(self, item):\n",
    "\n",
    "        # Parent item list\n",
    "        parent = set(item[0][0])\n",
    "        \n",
    "        # Child item list\n",
    "        if(isinstance(item[1][0] , str)):\n",
    "            child  = set([item[1][0]])\n",
    "        else:\n",
    "            child  = set(item[1][0])\n",
    "        # Parent and Child support values\n",
    "        parentSupport = item[0][1]\n",
    "        childSupport = item[1][1]\n",
    "        # Finds the item set confidence is going to be found\n",
    "\n",
    "        support = (parentSupport / childSupport)*100\n",
    "\n",
    "        return list([ list(child) ,  list(parent.difference(child)) , support ])\n",
    "\n",
    "        \n",
    "# Example ((('x10', 'x3', 'x6', 'x7', 'x9'), 1), (('x10', 'x3', 'x7'), 1))\n",
    "calcuItems = baseRdd.cartesian(baseRdd)\n",
    "\n",
    "# Create Filter Object\n",
    "ff = Filter()\n",
    "\n",
    "#deneme = calcuItems.map(lambda item: lens(item)) \n",
    "total = calcuItems.count()\n",
    "\n",
    "print('# : Aggregated support values preparing for the confidence calculatations')\n",
    "baseRddConfidence = calcuItems.filter(lambda item: ff.filterForConf(item , total))\n",
    "print('# : Aggregated support values are ready !')\n",
    "baseRddConfidence = baseRddConfidence.map(lambda item: ff.calculateConfidence(item))\n",
    "\n",
    "  \n",
    "# print(baseRddConfidence.collect())\n",
    "\n",
    "## Create an array with collected baseRddConfidence results\n",
    "result = baseRddConfidence.collect()\n",
    "\n",
    "## Create Data Frame\n",
    "confidenceTable = pd.DataFrame(data = result , columns=[\"Before\", \"After\" , \"Confidence\"])\n",
    "\n",
    "## Show data frame\n",
    "print(confidenceTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbce986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
